---
alwaysApply: false
---
Description: Pythonを使ってWebスクレイピングを行う際の、倫理的な配慮と技術的なベストプラクティスを定義します。スクレイピングに関するコード作成の質問があった場合に適用してください。

# 1. 倫理的なスクレイピング (良きWeb市民であるために)
- **考え方**: スクレイピングは、相手のサーバーにアクセスして情報を取得する行為です。無思慮なアクセスは相手サーバーに大きな負荷をかけ、業務妨害になりかねません。常に「お邪魔させていただきます」という謙虚な気持ちを持ち、敬意を払ったアクセスを心がけましょう。
- **必須のチェックリスト**:
    - **`robots.txt`の確認**: まず対象サイトの`robots.txt`（例: `https://example.com/robots.txt`）を確認し、スクレイピングが許可されているパスかを確認します。`Disallow`されているパスにはアクセスしません。
    - **`User-Agent`の設定**: スクレイパーが何者であるかをリクエストヘッダーに明記します。これにより、問題があった際にサイト管理者がスクレイパーを特定できます。
    - **リクエスト間の待機**: 連続でリクエストを送る際は、`time.sleep(1)`のように、最低でも1秒以上の待機時間を設けます。サーバーへの負荷を急激に高めないための、最も重要な配慮です。

- **AIへの指示**: 「PythonでWebスクレイピングをしたいんだけど、`robots.txt`を尊重し、`User-Agent`を設定し、リクエストごとに2秒の待機時間を入れるという、丁寧なスクリプトの雛形を書いてほしい。」

# 2. 適切なツールの選択
- **考え方**: サイトの作りによって、最適なツールは異なります。
- **静的なサイト**: ページのHTMLソースコードに直接データが含まれているサイト。
    - **ツール**: `requests`ライブラリでHTMLを取得し、`BeautifulSoup4`ライブラリで解析するのが基本です。
- **動的なサイト**: JavaScriptによってページが表示された後に、データが読み込まれるサイト。
    - **ツール**: `Playwright`や`Selenium`といった、実際にブラウザを操作するライブラリが必要です。これらはブラウザを起動するため、`requests`より処理は重くなります。
- **AIへの指示**: 「このURLのサイト、スクレイピングするには`BeautifulSoup`だけで十分かな？それとも`Playwright`が必要そうか、サイトの作りを見て判断してほしい。」

# 3. 堅牢なデータ抽出
- **考え方**: Webサイトのデザインは頻繁に変わるため、HTMLの構造も変化します。特定の`class`名だけに頼ると、すぐに動かなくなる可能性があります。
- **実践方法**:
    - `id`属性など、より変更されにくいセレクタを優先的に使います。
    - データが取得できなかった場合に、エラーで停止するのではなく、`None`を返すなどして処理を継続できるようにします。
- **AIへの指示**: 「`BeautifulSoup`で記事のタイトルを取得したいんだけど、もしタイトル要素が見つからなかった場合でも、エラーを出さずにスキップするような、安全なコードを書いて。」

# 4. データ品質保証 (Schema Validation)
- **考え方**: スクレイピングしたデータは「汚い（型が不揃い、欠損がある）」前提で扱います。取得したデータをそのままDBに入れるのではなく、必ず`zod`などのバリデーションライブラリを通して型と値を検証し、整形（クリーニング）してから使用します。
- **AIへの指示**: 「スクレイピングした商品データのオブジェクトを検証したい。`zod`を使って、価格が数値であること、商品名が空でないことをチェックするスキーマを作って。」

# 5. キャッシュ戦略 (Caching Strategy)
- **考え方**: 開発中に何度も同じサイトにアクセスするのは、相手サーバーへの迷惑であり、開発効率も下がります。一度取得したHTMLやレスポンスはローカル（ファイルやSQLite）にキャッシュし、開発中はキャッシュから読み込むようにしましょう。
- **AIへの指示**: 「`requests`で取得したHTMLを、URLのハッシュ値をファイル名として`./cache`フォルダに保存する仕組みを作って。2回目以降はファイルから読み込むようにしたい。」